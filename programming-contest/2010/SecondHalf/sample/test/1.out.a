[171] Notice that the numerator of (14) depends only on information about the current document, and that the number of terms in its sum is the number of actual terms in the document (not the total number of terms).
[119] That is, the fractional mix of topics in a document has a similar distribution independent of document length (it depends on number of passages on the same topic in the document, not their length).
[3] GaP is a factor model, that is it gives an approximate factorization of the document-term matrix into a product of matrices  and X.
[124] Intuitively, a fixed shape-factor GaP model should model well any documents where there are one or a few passages on the same theme in all the documents (there can of course be several passages on different themes in each document, GaP models those with the distinct xi ).
[130] This suggests an interesting as-yet unverified application of GaP models: The shape factor of the gamma for each theme provides information about the typical number of passages on that theme in each document the corpus.
[252] where q is a term appearing in the query, nq is the number of occurences in the query, p1 is the term's empirical probability (relative frequency) in the document, p2 is the term's probability as computed by GaP, and p3(q) is the probability of the term in the corpus (relative frequency in the corpus).
[107] The gamma distribution has another very nice property when used with this model: namely that it has a scale factor, and a scaled gamma distribution is still a gamma distribution.
[1] We present a probabilistic model for a document corpus that combines many of the desirable features of previous models.
[176] Because updating the X value for one document in the E-step requires only data that is local to one document, it has a very small memory footprint and easily fits in a CPU cache, usually in the L1 (fastest) cache.
[58] The factorization that GaP computes is a NMF (Nonnegative Matrix Factorization [10]) of the document corpus.
