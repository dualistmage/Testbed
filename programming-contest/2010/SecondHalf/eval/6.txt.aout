[70] Figure 1: Three topic models: LDA and two perspectives on TOT SYMBOL T D V Nd d z z zdi wdi tdi DESCRIPTION number of topics number of documents number of unique words number of word tokens in document d the multinomial distribution of topics specific to the document d the multinomial distribution of words specific to topic z the beta distribution of time specific to topic z the topic associated with the ith token in the document d the ith token in document d the timestamp associated with the ith token in the document d (in Figure 1(c))
[258] TOT's approach to modeling topic distributions conditioned on time stamp--based on multiple time-generating Betas, inverted with Bayes rule-- has the dual advantages of a relatively simple, easy-to-fit parameterization, while also offering topic distributions with a flexibility that would be more difficult to achieve with a direct, non-inverted parameterization, (i.e. one generating topic distributions directly conditioned on time, without Bayes-rule inversion).
[31] We fit the model parameters according to a generative model in which a per-document multinomial distribution over topics is sampled from a Dirichlet, then for each word occurrence we sample a topic; next a per-topic multinomial generates the word, and a per-topic Beta distribution generates the document's time stamp.
[3] Unlike other recent work that relies on Markov assumptions or discretization of time, here each topic is associated with a continuous distribution over timestamps, and for each generated document, the mixture distribution over topics is influenced by both word co-occurrences and the document's timestamp.
[47] In comparison to more complex alternatives, the relative simplicity of TOT is a great advantage--not only for the relative ease of understanding and implementing it, but also because this approach can in the future be naturally injected into other more richly structured topic models, such as the Author-Recipient-Topic model to capture changes in social network roles over time [10], and the Group-Topic model to capture changes in group formation over time [18].
[56] In its generative process, for each document d, a multinomial distribution d over topics is randomly sampled from a Dirichlet with parameter , and then to generate each word, a topic zdi is chosen from this topic distribution, and a word, wdi , is generated by randomly sampling from a topic-specific multinomial distribution zdi .
[134] Second, many other models take the view that the "meaning" (or word associations) of a topic changes over time; instead, in TOT we can rely on topics themselves as constant, while topic co-occurrence patterns change over time.
[100] Given a document, we predict its timestamp by choosing the discretized timestamp that maximizes the posterior which is calculated by multiplying the timestamp probability of all word tokens from their corresponding topic-wise Q Beta distributions over time, that is, arg maxt Nd p(t|zi ). i=1 It is also interesting to consider obtaining a distribution over topics, conditioned on a timestamp.
[95] An alternative generative process description of TOT, (better suited to generate an unseen document), is one in which a single timestamp is associated with each document, generated by rejection or importance sampling, from a mixture of per-topic Beta distributions over time with mixtures weight as the per-document d over topics.
[16] This paper presents Topics over Time (TOT), a topic model that explicitly models time jointly with word cooccurrence patterns.
